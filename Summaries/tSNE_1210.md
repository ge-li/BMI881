# t-SNE

### [van der Maaten & Hinton (2008)](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)

##### Title: Visualizing Data using t-SNE

##### Author: Laurens van der Maaten & Geoffrey Hinton

In this paper, the authors introduce a data visualization technique called "t-SNE" that extends the Stochastic Neighbor Embedding (SNE), which tries to assign low-dimensional coordinates to high-dimensional datapoints by minimizing the Kullback-Leibler divergences of the neighboring conditional probabilities between high and low dimensions. A stochastic gradient descent method with momentum is used in this optimization problem. t-SNE differs from SNE in two ways: (1) it uses a symmetrized cost function with simpler gradients; (2) it uses a Student-t distribution replacing the Gaussian to calculate conditional probabilities (similarities) in low-dimensional space. Consequently, t-SNE alleviate the crowding problem of SNE and is easier to optimize. The authors then demonstrate t-SNE on many data sets, achieving better-looking results comparing to other dimension-reduction visualization techniques. 

My thoughts:

* I think the motivation of SNE is clear. It is an unsupervised clustering method. First, it generates a similarity matrix from the original high-dimensional data. Next, it randomly samples low-dimensional datapoints as a starting point. Then iteratively, computing a low-dimensional version similarity matrix, defining a differentiable loss function between these two versions of matrices, and use stochastic gradient descent to minimize the loss function, so that the two similarity matrix resembles after many iterations. However, this procedure is based on the similarity matrix it produced from the high-dimensional data in the first place with the assumptions of Gaussian and Euclidean Space. The optimization is really trying to find some low-dimensional datapoints with similar similarity matrix. **In other words, the new datapoints don't add more knowledge to how similar original datapoints are with the same assumptions.**
* t-SNE improves SNE aesthetically and computationally. Aesthetically because it enforces many heuristic properties we wish the low-dimensional correspondents to have. Computationally because the new formulation of cost and conditional probabilities makes t-SNE an easier optimization problem. 
* The authors mentioned autoencoder in the discussion. In fact, that's my immediate reaction when I finished reading section 2 which introduces SNE. I believe the later important variational autoencoder combines the flavor of the two. Ref: [Auto-encoding variational bayes](https://arxiv.org/abs/1312.6114) 
* [Maximizing likelihood is equivalent to minimizing KL-Divergence](http://wiseodd.github.io/techblog/2017/01/26/kl-mle/)



